{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rdflib as rdf\n",
    "#import csv for reading csv files\n",
    "import csv\n",
    "#import for reading XLS data dictionary file\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from glob import glob\n",
    "import tarfile\n",
    "import dicom as dcm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = rdf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xml', rdflib.term.URIRef(u'http://www.w3.org/XML/1998/namespace')),\n",
       " ('rdf', rdflib.term.URIRef(u'http://www.w3.org/1999/02/22-rdf-syntax-ns#')),\n",
       " ('xsd', rdflib.term.URIRef(u'http://www.w3.org/2001/XMLSchema#')),\n",
       " ('rdfs', rdflib.term.URIRef(u'http://www.w3.org/2000/01/rdf-schema#'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nidash = rdf.Namespace(\"http://nidm.nidash.org#\")\n",
    "prov = rdf.Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "ncit = rdf.Namespace(\"http://ncitt.ncit.nih.gov/\")\n",
    "fbirn = rdf.Namespace(\"http://www.birncommunity.org/collaborators/function-birn/\")\n",
    "xsd = rdf.Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "rdfs = rdf.Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "foaf = rdf.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "vc = rdf.Namespace(\"http://www.w3.org/2006/vcard/ns#\")\n",
    "dicom = rdf.Namespace(\"http://neurolex.org/wiki/Category:DICOM_term/\")\n",
    "dct = rdf.Namespace(\"http://purl.org/dc/terms/\")\n",
    "dctypes = rdf.Namespace(\"http://purl.org/dc/dcmitype/\")\n",
    "dcat = rdf.Namespace(\"http://www.w3.org/ns/dcat#\")\n",
    "nfo = rdf.Namespace(\"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo#\")\n",
    "list(g.namespaces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xml', rdflib.term.URIRef(u'http://www.w3.org/XML/1998/namespace')),\n",
       " ('fbirn',\n",
       "  rdflib.term.URIRef(u'http://www.birncommunity.org/collaborators/function-birn/')),\n",
       " ('vc', rdflib.term.URIRef(u'http://www.w3.org/2006/vcard/ns#')),\n",
       " ('ncit', rdflib.term.URIRef(u'http://ncitt.ncit.nih.gov/')),\n",
       " ('rdfs', rdflib.term.URIRef(u'http://www.w3.org/2000/01/rdf-schema#')),\n",
       " ('nfo',\n",
       "  rdflib.term.URIRef(u'http://www.semanticdesktop.org/ontologies/2007/03/22/nfo#')),\n",
       " ('prov', rdflib.term.URIRef(u'http://www.w3.org/ns/prov#')),\n",
       " ('nidash', rdflib.term.URIRef(u'http://nidm.nidash.org#')),\n",
       " ('dctypes', rdflib.term.URIRef(u'http://purl.org/dc/dcmitype/')),\n",
       " ('rdf', rdflib.term.URIRef(u'http://www.w3.org/1999/02/22-rdf-syntax-ns#')),\n",
       " ('foaf', rdflib.term.URIRef(u'http://xmlns.com/foaf/0.1/')),\n",
       " ('xsd', rdflib.term.URIRef(u'http://www.w3.org/2001/XMLSchema#')),\n",
       " ('dicom',\n",
       "  rdflib.term.URIRef(u'http://neurolex.org/wiki/Category:DICOM_term/')),\n",
       " ('dcat', rdflib.term.URIRef(u'http://www.w3.org/ns/dcat#')),\n",
       " ('dct', rdflib.term.URIRef(u'http://purl.org/dc/terms/'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.bind('nidash', nidash)\n",
    "g.bind('prov', prov)\n",
    "g.bind('ncit', ncit)\n",
    "g.bind('fbirn', fbirn)\n",
    "g.bind('xsd',xsd)\n",
    "g.bind('rdfs',rdfs)\n",
    "g.bind('foaf',foaf)\n",
    "g.bind('vc',vc)\n",
    "g.bind('dicom',dicom)\n",
    "g.bind('dct',dct)\n",
    "g.bind('dctypes', dctypes)\n",
    "g.bind('dcat', dcat)\n",
    "g.bind('nfo',nfo)\n",
    "list(g.namespaces())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Functions for AssessmentOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nidm_add_elements(g,line,assessment_name, variable_name_id, question_id, type_id):\n",
    "    #Create new node for each element with metadata consistent with NIDM-Experiment assessment acquistion\n",
    "    g.add((nidash[line[variable_name_id]], rdf.RDF.type, nidash[\"DataElement\"]))\n",
    "    g.add((nidash[line[variable_name_id]], rdf.RDF.type, rdf.RDF.Property))\n",
    "    g.add((nidash[line[variable_name_id]], prov[\"label\"], rdf.Literal(line[variable_name_id])))\n",
    "    if (type_id in line.keys()):\n",
    "        g.add((nidash[line[variable_name_id]], nidash[\"DataType\"], rdf.Literal(line[type_id])))\n",
    "    if (question_id in line.keys()):\n",
    "        g.add((nidash[line[variable_name_id]], nidash[\"Question\"], rdf.Literal(line[question_id])))\n",
    "    if ('SCORESEQ' in line.keys()):\n",
    "        g.add((nidash[line[variable_name_id]], nidash[\"Sequence\"], rdf.Literal(line[\"SCORESEQ\"])))   \n",
    "    #make association with assessment\n",
    "    g.add((nidash[assessment_name],prov[\"hadMember\"], nidash[line[variable_name_id]]))\n",
    "def nidm_create_assessment(g, assessment_name):\n",
    "    g.add((nidash[assessment_name], rdf.RDF.type, nidash[\"DataStructure\"]))\n",
    "    g.add((nidash[assessment_name], rdf.RDF.type, prov[\"Entity\"]))\n",
    "    g.add((nidash[assessment_name],prov[\"label\"],rdf.Literal(assessment_name)))\n",
    "def nidm_add_codedproperty(g,line,valueset_id, codedvalue_id, score_code_id, score_label_id):\n",
    "    codedvalue_uri = safe_uri(codedvalue_id)\n",
    "    g.add((nidash[codedvalue_uri], rdf.RDF.type, nidash[\"CodedProperty\"]))\n",
    "    g.add((nidash[codedvalue_uri], rdf.RDF.type, rdf.RDF.Property))\n",
    "    g.add((nidash[codedvalue_uri],nidash[\"code\"], rdf.Literal(line[score_code_id])))\n",
    "    g.add((nidash[codedvalue_uri], prov[\"label\"], rdf.Literal(line[score_label_id])))\n",
    "    g.add((nidash[valueset_id], prov[\"hadMember\"], nidash[codedvalue_uri]))\n",
    "def safe_uri(string):\n",
    "    return string.strip().replace(\" \",\"_\").replace(\"-\", \"_\").replace(\",\", \"_\").replace(\"(\", \"_\").replace(\")\",\"_\").replace(\"'\",\"_\").replace(\"/\", \"_\")\n",
    "def nidm_create_assessment_acquisition_object(g, object_id, assessment_type):\n",
    "    #g.add((nidash[object_id], rdf.RDF.type, nidash[\"AcquisitionObject\"]))\n",
    "    g.add((nidash[object_id], rdf.RDF.type, nidash[assessment_type]))\n",
    "    g.add((nidash[object_id], rdf.RDF.type, prov[\"Entity\"]))\n",
    "    g.add((nidash[object_id], rdf.RDF.type, nidash[\"Assessment\"]))\n",
    "def nidm_add_elements_assessment_acquisition_object(g, object_id, element, value):\n",
    "    g.add((nidash[object_id], nidash[element], rdf.Literal(value)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Functions for ExperimentOM - Investigation Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nidm_create_investigation(g, uid, expid):\n",
    "    g.add((nidash[uid], rdf.RDF.type, dctypes[\"Dataset\"]))\n",
    "    g.add((nidash[uid], rdf.RDF.type, nidash[\"Investigation\"]))\n",
    "    g.add((nidash[uid], rdf.RDF.type, prov[\"Entity\"]))\n",
    "    g.add((nidash[uid], fbirn[\"ExperimentID\"], rdf.Literal(expid, lang='en')))\n",
    "def nidm_add_investigation_metadata(g, uid, name, description, baseuri, storagetype):\n",
    "    g.add((nidash[uid], dct[\"title\"], rdf.Literal(name, lang='en')))\n",
    "    g.add((nidash[uid], dct[\"description\"], rdf.Literal(description, lang='en')))\n",
    "    g.add((nidash[uid], dct[\"publisher\"], rdf.URIRef(baseuri)))\n",
    "    g.add((nidash[uid], dcat[\"accessURL\"], rdf.URIRef(baseuri)))\n",
    "    g.add((nidash[uid], fbirn[\"StorageType\"], rdf.Literal(storagetype, lang='en')))\n",
    "def nidm_add_role(g, uid, role):\n",
    "    g.add((uid, rdf.RDF.type, prov[\"Role\"]))\n",
    "    g.add((uid, prov[\"label\"], rdf.Literal(role)))\n",
    "def nidm_add_investigation_PI(g, uid, expid, first, last, email):\n",
    "    g.add((nidash[uid], rdf.RDF.type, prov[\"Person\"]))\n",
    "    g.add((nidash[uid], foaf[\"givenName\"], rdf.Literal(first)))\n",
    "    g.add((nidash[uid], foaf[\"familyName\"], rdf.Literal(last)))\n",
    "    g.add((nidash[uid], vc[\"email\"], rdf.Literal(email)))\n",
    "    #connect Person to Experiment and add Role\n",
    "    nidm_add_role(g, nidash[\"PI\"], \"Principle Investigator\")\n",
    "    g.add((nidash[uid], prov[\"hadRole\"], nidash[\"PI\"]))\n",
    "    g.add((nidash[uid], prov[\"wasAssociatedWith\"], nidash[expid]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#experiment and person dictionaries for lookup\n",
    "exp_dict = {}\n",
    "exp_collection_dict = {}\n",
    "person_dict = {}\n",
    "#Create investigation, parse metadata\n",
    "variables = pd.read_csv(\"./HID_Download/phaseIII_experiment_info.csv\")\n",
    "#iterate over the variables in the experiment info export\n",
    "for index, row in variables.iterrows():\n",
    "    #create UUID\n",
    "    expid = \"Investigation_\" + str(uuid.uuid1())\n",
    "    #keep UUID -> uniqueid mappings for experiments in dictionary in case there are multiple experiments\n",
    "    exp_dict[expid] = row[\"uniqueid\"]\n",
    "    #add investigation using HID experiment ID\n",
    "    nidm_create_investigation(g, expid,exp_dict[expid])\n",
    "    #add other metadata from file\n",
    "    nidm_add_investigation_metadata(g, expid, row[\"name\"], row[\"description\"],row[\"baseuri\"],row[\"storagetype\"])\n",
    "\n",
    "    #create an investigation collection / investigation activity\n",
    "    #collectid = \"InvestigationCollection_\" + str(uuid.uuid1())\n",
    "    #store URL for later use\n",
    "    #exp_collection_dict[str(row[\"uniqueid\"])] = collectid\n",
    "    \n",
    "    activityid = \"InvestigationActivity_\"+str(uuid.uuid1())\n",
    "    #g.add((nidash[collectid], rdf.RDF.type, prov[\"Collection\"]))\n",
    "    g.add((nidash[activityid], rdf.RDF.type, prov[\"Activity\"]))\n",
    "    #add label for debugging/model evaluation\n",
    "    g.add((nidash[expid],prov[\"label\"], rdf.Literal(\"Investigation Collection\")))\n",
    "    g.add((nidash[activityid],prov[\"label\"], rdf.Literal(\"Investigation Process Activity\")))\n",
    "    #associate investigation entity with activity\n",
    "    g.add((nidash[expid],prov[\"wasGeneratedBy\"], nidash[activityid]))\n",
    "    \n",
    "    #Create PI person, parse metadata\n",
    "    person_variables = pd.read_csv(\"./HID_Download/phaseIII_PI_info.csv\")\n",
    "    for person_index, person_row in person_variables.iterrows():\n",
    "        #create UUID\n",
    "        PIid = \"Person_\" + str(uuid.uuid1())\n",
    "        #keep UUID -> uniqueid mappings for persons in dictionary in case there are multiple person\n",
    "        person_dict[PIid] = person_row[\"uniqueid\"]\n",
    "        #add PI \n",
    "        nidm_add_investigation_PI(g, PIid, expid, person_row[\"first_name\"], person_row[\"last_name\"], person_row[\"email\"])\n",
    "        #associate person with investigation activity\n",
    "        g.add((nidash[activityid],prov[\"wasAssociatedWith\"], nidash[PIid]))\n",
    "        #associate investigation collection with person\n",
    "        g.add((nidash[expid],prov[\"wasAttributedTo\"], nidash[PIid]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Image series (DICOM) download parsing/conversion to NIDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image series functions\n",
    "def nidm_add_session_person(g, uid, subjid):\n",
    "    g.add((nidash[uid], rdf.RDF.type, prov[\"Person\"]))\n",
    "    g.add((nidash[uid], ncit[\"subjectID\"], rdf.Literal(subjid)))\n",
    "    nidm_add_role(g, nidash[\"Participant\"], \"Participant\")\n",
    "    g.add((nidash[uid], prov[\"hadRole\"], nidash[\"Participant\"]))\n",
    "def nidm_add_scanner(g, uid, dicom_hdr):\n",
    "    g.add((nidash[scanner_id], rdf.RDF.type, prov[\"Agent\"]))\n",
    "    g.add((nidash[scanner_id], dicom[\"Manufacturer\"], rdf.Literal(ds[0x0008, 0x0070].value)))\n",
    "    g.add((nidash[scanner_id], dicom[\"ManufacturerModelName\"], rdf.Literal(ds[0x0008,0x1090].value)))\n",
    "    g.add((nidash[scanner_id], dicom[\"MagneticFieldStrength\"], rdf.Literal(ds[0x0018,0x0087].value)))\n",
    "    g.add((nidash[scanner_id], dicom[\"DeviceSerialNumber\"], rdf.Literal(ds[0x0018,0x1000].value)))\n",
    "    g.add((nidash[scanner_id], dicom[\"SoftwareVersion\"], rdf.Literal(ds[0x0018,0x1020].value)))\n",
    "def nidm_add_dicom_metadata(g, uid, dicom_hdr):\n",
    "    g.add((nidash[uid], dicom[\"ScanningSequence\"], rdf.Literal(ds[0x0018,0x0020].value)))\n",
    "    g.add((nidash[uid], dicom[\"SequenceVariant\"], rdf.Literal(ds[0x0018,0x0021].value)))\n",
    "    g.add((nidash[uid], dicom[\"ScanOptions\"], rdf.Literal(ds[0x0018,0x0022].value)))\n",
    "    g.add((nidash[uid], dicom[\"MRAcquisitionType\"], rdf.Literal(ds[0x0018,0x0023].value)))\n",
    "    g.add((nidash[uid], dicom[\"SequenceName\"], rdf.Literal(ds[0x0018,0x0024].value)))\n",
    "    g.add((nidash[uid], dicom[\"AngioFlag\"], rdf.Literal(ds[0x0018,0x0025].value)))\n",
    "    g.add((nidash[uid], dicom[\"SliceThickness\"], rdf.Literal(ds[0x0018,0x0050].value)))\n",
    "    g.add((nidash[uid], dicom[\"RepetitionTime\"], rdf.Literal(ds[0x0018,0x0080].value)))\n",
    "    g.add((nidash[uid], dicom[\"EchoTime\"], rdf.Literal(ds[0x0018,0x0081].value)))\n",
    "    g.add((nidash[uid], dicom[\"NumberofAverages\"], rdf.Literal(ds[0x0018,0x0083].value)))\n",
    "    g.add((nidash[uid], dicom[\"ImagingFrequency\"], rdf.Literal(ds[0x0018,0x0084].value)))\n",
    "    g.add((nidash[uid], dicom[\"ImagedNucleus\"], rdf.Literal(ds[0x0018,0x0085].value)))\n",
    "    g.add((nidash[uid], dicom[\"EchoNumber\"], rdf.Literal(ds[0x0018,0x0086].value)))\n",
    "    g.add((nidash[uid], dicom[\"MagneticFieldStrength\"], rdf.Literal(ds[0x0018,0x0087].value)))\n",
    "    g.add((nidash[uid], dicom[\"SpacingBetweenSlices\"], rdf.Literal(ds[0x0018,0x0088].value)))\n",
    "    g.add((nidash[uid], dicom[\"NumberofPhaseEncodingSteps\"], rdf.Literal(ds[0x0018,0x0089].value)))\n",
    "    g.add((nidash[uid], dicom[\"EchoTrainLength\"], rdf.Literal(ds[0x0018,0x0091].value)))\n",
    "    g.add((nidash[uid], dicom[\"PercentSampling\"], rdf.Literal(ds[0x0018,0x0093].value)))\n",
    "    g.add((nidash[uid], dicom[\"PercentPhaseFieldofView\"],rdf.Literal(ds[0x0018,0x0094].value)))\n",
    "    g.add((nidash[uid], dicom[\"PixelBandwidth\"],rdf.Literal(ds[0x0018,0x0095].value)))\n",
    "    g.add((nidash[uid], dicom[\"ProtocolName\"],rdf.Literal(ds[0x0018,0x1030].value)))\n",
    "    g.add((nidash[uid], dicom[\"TransmitCoilName\"],rdf.Literal(ds[0x0018,0x1251].value)))\n",
    "    g.add((nidash[uid], dicom[\"AcquisitionMatrix\"],rdf.Literal(ds[0x0018,0x1310].value)))\n",
    "    g.add((nidash[uid], dicom[\"InplanePhaseEncodingDirection\"],rdf.Literal(ds[0x0018,0x1312].value)))\n",
    "    g.add((nidash[uid], dicom[\"FlipAngle\"], rdf.Literal(ds[0x0018,0x1314].value)))\n",
    "    g.add((nidash[uid], dicom[\"VariableFlipAngleFlag\"], rdf.Literal(ds[0x0018,0x1315].value)))\n",
    "    g.add((nidash[uid], dicom[\"SAR\"], rdf.Literal(ds[0x0018,0x1316].value)))\n",
    "    g.add((nidash[uid], dicom[\"dB_dt\"], rdf.Literal(ds[0x0018,0x1318].value)))\n",
    "    g.add((nidash[uid], dicom[\"PatientPosition\"], rdf.Literal(ds[0x0018,0x5100].value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#traverse directory structure, store visit/study metadata parsed from directory names per\n",
    "#FBIRN hierarchy, parse DICOM metadata at lowest level and format according to NIDM\n",
    "root_dir = \"./HID_Download/\"\n",
    "subj_dict={}\n",
    "#session_dict={}\n",
    "\n",
    "#create visit dictionary\n",
    "visit_dict = {}  \n",
    "for dname in os.listdir(root_dir):\n",
    "    if ((os.path.isdir(os.path.join(root_dir,dname))) and (not dname.startswith('.'))):\n",
    "        #highest level directory contains subj-dirs with subjectID\n",
    "        #grab subjectID and store in NIDM-Experiment graph\n",
    "        #subject ID should be unique so can drop the UUID...and then we don't need a dictionary\n",
    "        #subj_uid = dname + \"_\" + str(uuid.uuid1())\n",
    "        subj_uid = str(dname)\n",
    "        subj_dict[dname] = subj_uid\n",
    "        nidm_add_session_person(g,subj_uid, dname)\n",
    "    \n",
    "        ###VISIT COLLECTION TAKES PLACE OF THIS CODE....####\n",
    "        #create session activity and collection\n",
    "        #create an investigation collection / investigation activity\n",
    "        #collectid = \"SessionCollection_\" + str(uuid.uuid1())\n",
    "        \n",
    "        #key session collection by subjectID+visit_number\n",
    "        #session_dict[dname + \"_\" + tokens[2]] = collectid\n",
    "        #activityid = \"SessionActivity_\"+str(uuid.uuid1())\n",
    "        #g.add((nidash[collectid], rdf.RDF.type, prov[\"Collection\"]))\n",
    "        #g.add((nidash[activityid], rdf.RDF.type, prov[\"Activity\"]))\n",
    "        \n",
    "        #associate person with these session collections/activities\n",
    "        #g.add((nidash[collectid], prov[\"wasAttributedTo\"], nidash[subj_uid]))\n",
    "        #g.add((nidash[activityid], prov[\"wasAssociatedWith\"], nidash[subj_uid]))\n",
    "\n",
    "        #now start traversing, next level is visit level\n",
    "      \n",
    "        for visitname in os.listdir(os.path.join(root_dir,dname)):\n",
    "            if ((os.path.isdir(os.path.join(root_dir,dname,visitname))) and (not visitname.startswith('.'))):\n",
    "                #next sub dirs are visits\n",
    "                #parse visit info from directory name\n",
    "                tokens=visitname.split(\"__\")\n",
    "                #token[0] = visitname, token[1] = siteid, token[2] = visit number\n",
    "                #add visit entity and associate wtih session collection\n",
    "                #visit unique id is \"Visit_\"+visit number+visit name + subjectID\n",
    "                visitid = \"AcquisitionCollection_\" + str(tokens[2]) + \"_\" + str(tokens[0]) + \"_\" + dname\n",
    "                visit_dict[dname + \"_\" + tokens[2]] = visitid\n",
    "                g.add((nidash[visitid], rdf.RDF.type, prov[\"Entity\"]))\n",
    "                g.add((nidash[visitid], rdf.RDF.type, nidash[\"Session\"]))\n",
    "                g.add((nidash[visitid], ncit[\"VisitNum\"], rdf.Literal(tokens[2])))\n",
    "                g.add((nidash[visitid], ncit[\"StudySiteNumber\"], rdf.Literal(tokens[1])))\n",
    "                g.add((nidash[visitid], prov[\"label\"], rdf.Literal(tokens[0])))\n",
    "                #g.add((nidash[collectid], prov[\"hadMember\"], nidash[visitid]))\n",
    "                #visit collection is the \"session\" collection, attribute to subject\n",
    "                g.add((nidash[visitid], prov[\"wasAttributedTo\"], nidash[subj_dict[dname]]))\n",
    "                \n",
    "                #create scanner dictionary for use in scanners associated with series acquisitions\n",
    "                scanner_dict={}\n",
    "                \n",
    "                #Study Level\n",
    "                for studyname in os.listdir(os.path.join(root_dir,dname, visitname)):\n",
    "                    #Skip Levels until Series Level...where do we put multiple studies in model?\n",
    "                    if ((os.path.isdir(os.path.join(root_dir,dname,visitname,studyname))) and (not studyname.startswith('.'))):\n",
    "                        for seriesname in os.listdir(os.path.join(root_dir,dname, visitname,studyname)):\n",
    "                            if ((os.path.isdir(os.path.join(root_dir,dname,visitname,studyname,seriesname))) and (not seriesname.startswith('.'))):\n",
    "                                #create acquisition activity\n",
    "                                session_act_id = \"AcquisitionActivity\" + str(uuid.uuid1())     \n",
    "                                g.add((nidash[session_act_id], rdf.RDF.type, prov[\"Activity\"]))\n",
    "                                \n",
    "                                      \n",
    "                                #get some metadata about the activity by parsing the DICOM tags\n",
    "                                fname = os.path.join(root_dir,dname,visitname,studyname,seriesname,\"Native\",\"Original__0001\",\"DICOM.tar.gz\")\n",
    "                                tar = tarfile.open(fname, \"r:gz\")\n",
    "                                try:\n",
    "                                    tar_contents = tar.getmembers()\n",
    "                                    for files in tar_contents:\n",
    "                                        if \".dcm\" in files.name:\n",
    "                                            dcmfilename = files.name\n",
    "                                            #print dcmfilename + \"found!\"\n",
    "                                            break\n",
    "                                     \n",
    "                                    dcmfile = tar.extractfile(dcmfilename)\n",
    "                                        \n",
    "                                    #for member in tar.getmembers():\n",
    "                                    #    print member\n",
    "                                    #now get DICOM header tags\n",
    "                                    ds = dcm.read_file(dcmfile)\n",
    "                                    #add acquisition time to activity object\n",
    "                                    g.add((nidash[session_act_id], prov[\"startedAtTime\"], rdf.Literal(ds[0x0008,0x0031].value)))\n",
    "                                    #create scanner unless it already exists then just associate with acquisition object\n",
    "                                    scanner_key = str(ds[0x0008, 0x0070].value)+ \"_\"+str(ds[0x0008,0x1090].value)+\"_\"+str(ds[0x0018,0x0087].value)\n",
    "                                    if scanner_dict.has_key(scanner_key):\n",
    "                                        #associate with acquisition activity and scan entity\n",
    "                                        g.add((nidash[session_act_id], prov[\"wasGeneratedBy\"],nidash[scanner_dict[scanner_key]] ))\n",
    "                                        \n",
    "                                    else:\n",
    "                                        #create scanner agent\n",
    "                                        #key is manufacturer+model+field_strength\n",
    "                                        scanner_id = scanner_key + str(uuid.uuid1())\n",
    "                                        scanner_dict[scanner_key] = scanner_id\n",
    "                                        nidm_add_scanner(g, scanner_id, ds)\n",
    "                                        #associate scanner agent with acquisition activity\n",
    "                                        g.add((nidash[session_act_id], prov[\"Used\"], nidash[scanner_id]))\n",
    "                                        #create scan entity\n",
    "                                        session_id = \"AcquistionObject_\" + str(uuid.uuid1())\n",
    "                                        g.add((nidash[session_id], rdf.RDF.type, prov[\"Entity\"]))\n",
    "                                        #add some metadata....what metadata do we add?\n",
    "                                        nidm_add_dicom_metadata(g, session_id, ds)\n",
    "                                        #here we need to annotate acquisition object as an anatomical or structural\n",
    "                                        #scan.... \n",
    "                                        #for FBIRN the directory names will tell us but for other data sets\n",
    "                                        #we should match the DICOM tags to a classification scheme\n",
    "                                        if ( ((seriesname.lower().find(\"t1\"))>0 ) or ((seriesname.lower().find(\"t2\"))>0)):\n",
    "                                            #add attribute for anatomical scan\n",
    "                                            g.add((nidash[session_id], rdf.RDF.type, nidash[\"MRAnatomical\"])) \n",
    "                                        else:\n",
    "                                            g.add((nidash[session_id], rdf.RDF.type, nidash[\"MRFunctional\"])) \n",
    "                             \n",
    "                                        \n",
    "                                        #add filename/location to entity\n",
    "                                        g.add((nidash[session_id], nfo[\"filename\"], rdf.Literal(\"DICOM.tar.gz\")))\n",
    "                                        g.add((nidash[session_id], prov[\"atLocation\"], rdf.Literal(os.path.join(dname,visitname,studyname,seriesname,\"Native\",\"Original__0001\"))))\n",
    "                                        \n",
    "                                        #create association with anatomical activity\n",
    "                                        g.add((nidash[session_id], prov[\"wasGeneratedBy\"], nidash[session_act_id]))\n",
    "                                except (tarfile.TarError,tarfile.ReadError,tarfile.CompressionError,tarfile.StreamError,tarfile.ExtractError,tarfile.HeaderError):\n",
    "                                    print \"Error opening DICOM file\"+dcmfile\n",
    "                                   \n",
    "                                    \n",
    "                                \n",
    "                                        \n",
    "                                        \n",
    "                                              \n",
    "                                        \n",
    "                                \n",
    "                          \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assessment modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HID assessments are downloaded with filenames \"data_download_[AssessmentName].csv\" where [AssessmentName] matches the \n",
    "#name of the assessment in the data dictionary files.\n",
    "#For FBIRN phase III there are CMINDS assessments along with HID/tablet clinical assessments.  For the CMINDS assessments\n",
    "#each assessment has a separate XLS file with the data dictionary.  For the HID/tablet clinical assessments, all\n",
    "#assessments are in the same data dictionary XLS file...so there will be some complexity in modeling these...\n",
    "\n",
    "\n",
    "#Since we are going to store the data dictionaries in a separate TTL file, create a new graph\n",
    "#dd_graph = rdf.Graph()\n",
    "#changed to use the same file as the rest of the experiment data\n",
    "dd_graph = g\n",
    "#bind namespaces\n",
    "dd_graph.bind('nidash', nidash)\n",
    "dd_graph.bind('prov', prov)\n",
    "dd_graph.bind('ncit', ncit)\n",
    "dd_graph.bind('nidash', nidash)\n",
    "dd_graph.bind('fbirn', fbirn)\n",
    "\n",
    "#Find data_download_[*].csv and loop through them\n",
    "data_dictionary_dir = \"./HID_DataDictionary\"\n",
    "#Added acquired assessment directory for code clarity\n",
    "assessment_dir = root_dir\n",
    "\n",
    "#create dictionary of assessment UIDs\n",
    "assessment_dict = {}\n",
    "acquired_assessment_dict = {}\n",
    "\n",
    "#create dictionary of value set, coded properties\n",
    "coded_property_dict = {}\n",
    "\n",
    "#non-coded property variables\n",
    "assessment_variables_dict = {}\n",
    "\n",
    "#for dd_file in os.listdir(os.path.join(root_dir, \"data_download*\")):\n",
    "for dd_file in glob(os.path.join(root_dir, \"data_download*\")):\n",
    "    datadic_file = \"\"\n",
    "    assessment_name = \"\"\n",
    "\n",
    "    #do data dictionary modeling first so parse assessment name from dd_file and see if it's one of the CMINDS tests\n",
    "    tokens=dd_file.split(\"_\")\n",
    "    #tokens[1]==data, tokens[2]==download, tokens[3]==assessment_name.csv\n",
    "    #strip off .csv\n",
    "    tokens[3] = re.sub('\\.csv$','', tokens[3])\n",
    "\n",
    "    #look for matching file in data_dictionary_dir\n",
    "    for datadic in glob(os.path.join(data_dictionary_dir,\"*\")):\n",
    "        #note, CMINDS assessment data dictionaries are stored in separate files named by the assessment vs. FBIRN clinical\n",
    "        #data collected on the tablet are all stored in the same data dictionary file....so if we can't find a filename\n",
    "        #matching tokens[3] then it's not a CMINDS assessment\n",
    "        if (datadic.find(tokens[3])!= -1):\n",
    "            \n",
    "            #check if assessment name has already been encountered, if so use existing ID\n",
    "            #if assessment_dict.has_key(\"CMINDS_\"+ tokens[3]):\n",
    "            #    assessment_id = assessment_dict[\"CMINDS_\"+ tokens[3]]\n",
    "            \n",
    "            #Removed \"CMINDS\" string from these assessment keys (see previous 2 lines)...decided not to differentiate\n",
    "            #CMINDS assessments from tablet-based clinical assessments.  We could do this using an agent which is\n",
    "            #probably the sematically correct way since they were collected using different devices.\n",
    "            if assessment_dict.has_key(tokens[3]):\n",
    "                assessment_id = assessment_dict[tokens[3]]\n",
    "            \n",
    "            else:\n",
    "                #print \"found matching data dictionary: \" + datadic\n",
    "                #assessment_id = \"CMINDS_\"+ tokens[3] + \"_\" + str(uuid.uuid1())\n",
    "                #assessment_id = \"CMINDS_\"+ tokens[3]\n",
    "                #assessment_dict[\"CMINDS_\"+ tokens[3]] = assessment_id\n",
    "                \n",
    "                #this doesn't need to be a dictionary anymore since we ditched the UUIDs.  This dictionary\n",
    "                #was used to map assessment name -> assessment name + UUID\n",
    "                assessment_id = tokens[3]+\"DataStructure\"\n",
    "                assessment_dict[tokens[3]] = assessment_id\n",
    "            \n",
    "               \n",
    "            #create assessment entity\n",
    "            nidm_create_assessment(dd_graph, assessment_id)\n",
    "            \n",
    "            \n",
    "            #now open CMINDS data dictionary and start modeling\n",
    "            #Note, CMINDS data dictionaries are 1 sheet, no value-sets for these\n",
    "            xls = pd.ExcelFile(datadic)\n",
    "            #3rd sheet is summary variables\n",
    "            variables = xls.parse(2)\n",
    "            #iterate over the variables in the data dictionary\n",
    "            for index, row in variables.iterrows():\n",
    "                #add elements to RDF graph for variable definitions\n",
    "                nidm_add_elements(dd_graph,row,assessment_id, \"Variable Name\", \"Description\", \"Valid_Values\") \n",
    "                #store assessment variables in dictionary\n",
    "                assessment_variables_dict[row[\"Variable Name\"]] =row[\"Variable Name\"] \n",
    "                \n",
    "            \n",
    "                  \n",
    "        else:\n",
    "            #assessment not a CMINDS assessment so parse data dictionary stuff from HID PhaseIII data dictionary\n",
    "            xls = pd.ExcelFile(os.path.join(data_dictionary_dir,\"FBIRN_PhaseIII_Assessment_DataDictionary_20110330.xls\"))\n",
    "            variables = xls.parse(0)\n",
    "            value_sets = xls.parse(1)\n",
    "            #iterate over the variables in the data dictionary\n",
    "            for index, row in variables.iterrows():\n",
    "            \n",
    "                #check if assessment name has already been encountered, if so use existing ID\n",
    "                if assessment_dict.has_key(tokens[3]):\n",
    "                    assessment_id = assessment_dict[tokens[3]]\n",
    "                else:\n",
    "                    #print \"found matching data dictionary: \" + datadic\n",
    "                    #assessment_id = tokens[3] + \"_\" + str(uuid.uuid1())\n",
    "                    \n",
    "                    #this doesn't need to be a dictionary anymore since we ditched the UUIDs.  This dictionary\n",
    "                    #was used to map assessment name -> assessment name + UUID\n",
    "                    assessment_id = tokens[3]+\"DataStructure\"\n",
    "                    assessment_dict[tokens[3]] = assessment_id\n",
    "            \n",
    "                #create assessment entity\n",
    "                nidm_create_assessment(dd_graph, assessment_id)\n",
    "            \n",
    "                \n",
    "                #if assessment name in column A matches the assessment name in the data_download_[assessment name].csv\n",
    "                #filename then we'll start parsing the value set information from the other columns\n",
    "                if (row['Assessment Name'].find(tokens[3])!=-1):\n",
    "                    #print \"Found match: \" + tokens[3] + \" in \" + row['Assessment Name']\n",
    "                    #add elements to RDF graph for variable definitions\n",
    "                    nidm_add_elements(dd_graph,row,assessment_id, \"Data ID\", \"Question Text\",\"\") \n",
    "              \n",
    "                    #store assessment variables in list\n",
    "                    assessment_variables_dict[row[\"Data ID\"]] = row[\"Data ID\"]\n",
    "\n",
    "            \n",
    "                    #look for value sets in data dictionary that match the assessment name+variable name\n",
    "                    query_sets = value_sets[value_sets[\"Data ID\"].str.contains(row[\"Data ID\"])]\n",
    "                    #iterate over the value set rows and add to RDF graph\n",
    "                    for query_index, query_row in query_sets.iterrows():\n",
    "                        #print query_row['SCORECODE'] + \",\" + query_row['SCORELABEL'] \n",
    "                        #add attribute to coded data element for valueset\n",
    "                        #valueset_id = assessment_id+\"_\"+row[\"Data ID\"]+\"_ValueSet\"\n",
    "                        valueset_id = row[\"Data ID\"]+\"_ValueSet\"\n",
    "                        dd_graph.add((nidash[row[\"Data ID\"]], nidash[\"ValueSet\"], nidash[valueset_id]))\n",
    "                        #create value set collection\n",
    "                        dd_graph.add((nidash[valueset_id], rdf.RDF.type, prov[\"Collection\"]))\n",
    "                        dd_graph.add((nidash[valueset_id], rdf.RDF.type, nidash[\"ValueSet\"]))\n",
    "                        nidm_add_codedproperty(dd_graph, query_row, valueset_id ,row[\"Data ID\"] + \"_\" + str(query_row[\"SCORECODE\"]), \"SCORECODE\",\"SCORELABEL\" )\n",
    "                        #This if for finding a coded property URL from the acquired data.  If the assessment variable\n",
    "                        #is a coded-property then in the acquired assessment data entity instead of using the code literal \n",
    "                        #we use the coded-property URL by searching this dictionary \n",
    "                        coded_property_dict[row[\"Data ID\"]] = str(query_row[\"SCORECODE\"])\n",
    "    \n",
    "    #print\"Coded Property Dict\"\n",
    "    #print coded_property_dict\n",
    "    \n",
    "    #print \"Assessments variables list\"\n",
    "    #print assessment_variables_list\n",
    "    \n",
    "    #parse/model measured assessments\n",
    "    input_file = csv.DictReader(open(dd_file))\n",
    "    \n",
    "    #dictionary keys to skip during assessment data modeling\n",
    "    nondata_keys = ['SubjectID', 'SiteID', 'ExperimentID', 'VisitID', 'SegmentID']\n",
    "    \n",
    "    #print coded_property_dict\n",
    "    \n",
    "    #figure out which experiment, subj, site, and visit each row corresponds to and model assessment data\n",
    "    #match with assessment from data dictionary\n",
    "    if assessment_dict.has_key(tokens[3]):\n",
    "        assessment_id = assessment_dict[tokens[3]]\n",
    "    else:\n",
    "        #if this happens we won't be able to link the assessment data to a data dictionary class\n",
    "        #so let's just create an ad-hoc assessment in the data dictionary with no items unfortunately.  We could\n",
    "        #create an empirical data dictionary from the collected assessment data as we did with the SimpleData \n",
    "        #example: https://github.com/incf-nidash/nidm/tree/master/nidm/nidm-experiment/simpledata_example_ohbm_hack_2016\n",
    "        assessment_id = tokens[3]+\"DataStructure\"+\"_\"+str(uuid.uuid1())\n",
    "        nidm_create_assessment(dd_graph, assessment_id)\n",
    "        print \"ERROR: Assessment data found with no matching data dictionary class!!\"\n",
    "        print \"Creating assessment object in data dictionary with no elements. Consider creating empirical data dictionary!\"\n",
    "\n",
    " \n",
    "    #each row is a subject observation key'd by subjID, siteID, ExperimentID, and VisitID\n",
    "    for line in input_file:\n",
    "        #create assessment collection activity for each subject/project/visit/assessment\n",
    "        activity_uri = \"AcquisitionActivity_\"+line[\"SubjectID\"]+\"_\"+str(line[\"VisitID\"]).rjust(4,'0')\n",
    "        #add activity to graph\n",
    "        g.add((nidash[activity_uri], rdf.RDF.type, prov[\"Activity\"]))\n",
    "        #add label for debugging/model evaluation\n",
    "        g.add((nidash[activity_uri],prov[\"label\"], rdf.Literal(\" Assessment Data Collection Activity\")))\n",
    "        \n",
    "        #match data to correct experiment \n",
    "        if exp_collection_dict.has_key(str(line[\"ExperimentID\"])):\n",
    "            #print \"Found Experiment Collection Object\"\n",
    "            #associate acquired assessment with experiment acquisition activity\n",
    "            g.add((nidash[activity_uri], prov[\"wasAssociatedWith\"], nidash[exp_collection_dict[str(line[\"ExperimentID\"])]]))\n",
    "        #associate activity with subject agent\n",
    "        if subj_dict.has_key(str(line[\"SubjectID\"])):\n",
    "            #associated activity with subject\n",
    "            g.add((nidash[activity_uri], prov[\"wasAssociatedWith\"], nidash[subj_dict[str(line[\"SubjectID\"])]]))\n",
    "        else:\n",
    "            #create subject\n",
    "            subj_uid = str(line[\"SubjectID\"])\n",
    "            subj_dict[str(line[\"SubjectID\"])] = subj_uid\n",
    "            nidm_add_session_person(g,subj_uid, subj_uid)\n",
    "    \n",
    "        #associate activity with Visit\n",
    "        if visit_dict.has_key(str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')):\n",
    "            #print \"Found Visit Collection Object\"\n",
    "            #associate acitivity with visit\n",
    "            g.add((nidash[activity_uri], prov[\"wasAssociatedWith\"], nidash[visit_dict[str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')]]))\n",
    "        else:\n",
    "            #if not session_dict.has_key(str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')):\n",
    "                #create a session for this visit\n",
    "            #    collectid = \"SessionCollection_\" + str(uuid.uuid1())\n",
    "            #    session_dict[str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')] = collectid\n",
    "            \n",
    "            #add visit to session collection\n",
    "            #add a visit collection for this visit\n",
    "            visitid = \"AcquisitionCollection_\" + str(line[\"VisitID\"]).rjust(4,'0') + \"_\" + \"AssessmentAcquisition\" + \"_\" + str(line[\"SubjectID\"])\n",
    "            visit_dict[str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')] = visitid\n",
    "            g.add((nidash[visitid], rdf.RDF.type, prov[\"Entity\"]))\n",
    "            g.add((nidash[visitid], rdf.RDF.type, nidash[\"Session\"]))\n",
    "            g.add((nidash[visitid], ncit[\"VisitNum\"], rdf.Literal(line[\"VisitID\"])))\n",
    "            g.add((nidash[visitid], ncit[\"StudySiteNumber\"], rdf.Literal(line[\"SiteID\"])))\n",
    "            g.add((nidash[visitid], prov[\"label\"], rdf.Literal(\"Assessment Acquisition Visit\")))\n",
    "            g.add((nidash[visitid], rdf.RDF.type, nidash[\"Assessment\"]))\n",
    "        \n",
    "            #g.add((nidash[session_dict[str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')]], prov[\"hadMember\"], nidash[visitid]))\n",
    "            #associate subject with visit collection\n",
    "            #visit collection is the \"session\" collection, attribute to subject\n",
    "            g.add((nidash[visitid], prov[\"wasAttributedTo\"], nidash[subj_dict[str(line[\"SubjectID\"])]]))\n",
    "                \n",
    "            #associate assessment acquisition acitivity with visit\n",
    "            g.add((nidash[activity_uri], prov[\"wasAssociatedWith\"], nidash[visit_dict[str(line[\"SubjectID\"]) + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')]]))\n",
    "\n",
    "        #create an entity for the assessment data\n",
    "        acquired_assessment_id = tokens[3] + \"_\" + line[\"SubjectID\"] + \"_\" + str(line[\"VisitID\"]).rjust(4,'0')\n",
    "        nidm_create_assessment_acquisition_object(g, acquired_assessment_id, assessment_id)\n",
    "        #add assessment items to entity but need to skip subjID, siteID, experimentID, and visitID columns\n",
    "        \n",
    "        for items in input_file.fieldnames:\n",
    "            #if this key is not one of the non-data columns (see nondata_keys above) then it's data from the \n",
    "            #assessment\n",
    "            if items not in nondata_keys:\n",
    "                #check if variable_codedvalue is a coded property, if so pass the coded property URL instead of the actual value\n",
    "                #find assessment item variable name from compound assessmentname_variablename patterns in \n",
    "                #acquired assessment download files\n",
    "                #found=0\n",
    "                for key in coded_property_dict.keys():\n",
    "                    if (items.find(key) > 0):\n",
    "                        var_name_indx  = items.find(key)\n",
    "                        #add data to our entity where value is the URL of the coded-property\n",
    "                        \n",
    "                        nidm_add_elements_assessment_acquisition_object(g, acquired_assessment_id, items[var_name_indx:], nidash[items[var_name_indx:]] + \"_\" + safe_uri(coded_property_dict[key]))\n",
    "                        #found = 1\n",
    "                        #print \"items = \" + items + \" ,var_name_indx = \" + str(var_name_indx) + \" , key= \" + key + \" ,items[var_name_indx:] = \" + items[var_name_indx:] + \\\n",
    "                        #\" , found = \" + str(found)\n",
    "                     \n",
    "                        break\n",
    "                else:\n",
    "                    #this is not a coded property so just get variable name without extra compound name\n",
    "                    #stuff that appears in the acquired assessment download file\n",
    "                    for name in assessment_variables_dict.keys():\n",
    "                        if (items.find(name) > 0):\n",
    "                            #print \"non-coded property: \" + name\n",
    "                            #name is the variable name \n",
    "                            #add the data to our entity\n",
    "                            nidm_add_elements_assessment_acquisition_object(g, acquired_assessment_id, name, line[items])\n",
    "        \n",
    "        #add association with activity\n",
    "        g.add((nidash[acquired_assessment_id], prov[\"wasGeneratedBy\"], nidash[activity_uri]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print g.serialize(format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"FBIRNPhaseIII_Experiment.ttl\",'w') as f:\n",
    "    f.write(g.serialize(format='turtle'))\n",
    "#with open(\"FBIRNPhaseIII_DataDictionary.ttl\",'w') as f:\n",
    "#    f.write(dd_graph.serialize(format='turtle'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u2013' in position 461: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-48ddde6ec48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrdf2dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FBIRNPhaseIII_Experiment.dot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdf2dot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdf2dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FBIRNPhaseIII_DataDictionary.dot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdf2dot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdf2dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/rdflib-4.2.dev0-py2.7.egg/rdflib/tools/rdf2dot.pyc\u001b[0m in \u001b[0;36mrdf2dot\u001b[0;34m(g, stream, opts)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;34mu\"<font point-size='10' color='#6666ff'>%s</font></td>\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;34mu\"</tr>%s</table> > ] \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopstr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNODECOLOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2013' in position 461: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "from rdflib.tools import rdf2dot\n",
    "with open(\"FBIRNPhaseIII_Experiment.dot\",'w') as f:\n",
    "    s = rdf2dot.rdf2dot(g, f)\n",
    "#with open(\"FBIRNPhaseIII_DataDictionary.dot\",'w') as f:\n",
    "#    s = rdf2dot.rdf2dot(dd_graph, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
